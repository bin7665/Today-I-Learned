### RNN
- 장기 문맥 의존성: 관련 요소가 멀리 떨어져 있을 경우 제대로 작동 못 함.
- CNN에도 위와 같은 문제가 생기긴 하는데 RNN은 긴 입력 샘플이 자주 발생하기 때문에 CNN보다 문제가 더 심각함
- RNN에서는 이를 해결하기 위해 ReLU를 사용하지 못함. ReLU는 선형적으로 계속 증가하기 때문에 결과값이 너무 커질 수도 있다.
- LSTM(Long Short-Term Memory): 장기 기억을 위한 상태를 추가해서 사용한다.
- forget gate: 어떤 부분을 장기기억에서 지울지를 결정한다.
- input gate: 어떤 부분을 장기기억에 넣을지를 결정한다.
- output gate: 어떤 부분을 장기 기억에서 단기 기억에 추가할지를 결정한다.