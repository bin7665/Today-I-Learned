### Passport

- user 인증을 위해 사용하는 모듈
- 쿠키를 이용해서 인증을 할 수 있도록 도와준다.
- strategy: passport에서 사용하는 인증 전략

### Deep Learning 간단 복습

- 신경망: 여러 층의 퍼셉트론을 연결시키고 조합해서 주어진 입력값에 대한 판단을 하게 함
- 퍼셉트론: 입력값과 활성함수를 사용해 출력값을 다음으로 넘기는 가장 작은 단위
- MLP(Multi-Layer Perceptron): 좌표 평면 자체에 변화를 주면 되는데 은닉층을 주면 된다.
- BP(Back-Propagation): 경사하강법의 확장 개념, 가중치를 구하는 방법은 경사 하강법을 그대로 이용, 임의의 가중치를 선언하고 이 오차가 최소인 지점으로 조금씩 이동해서 미분했을 때 기울기가 0인 것을 찾음.
- TLU(Threshold Logic Unit): 입력값과 가중치를 곱한 것의 합에 step function(logistic, sigmoid)을 적용해서 사용하는 방식
- DNN(Deep Neural Network): 히든 레이어가 엄청 많은 것, 총 layer의 개수가 4개 이상일 때
- Epoch: BP training algorithm에서 한 번의 mini-batch를 수행하는 단위
- forward pass: 입력값을 통해 출력값을 만들어내는 과정, 예측값을 만들 때 사용되었던 모든 은닉층의 가중치 값, 계산값 등을 다 저장하고 있다.
- BP: 예측값을 생성(forward pass) -> 에러 판단 -> 각 연결에 대한 에러 기여도 판단(reverse pass) -> 연결 가중치를 조정(Gradient Descent)
- vanishing gradients problem: 가중치가 너무 작아서 reverse pass에서 값을 거의 변화하지 않는다. -> relu 사용(0이 문제) -> leaky relu, elu(< 0이 지수 함수)
- BN(Batch Normalization): input 값들을 0을 기준으로 정규화해줌, activation function 실행 전 또는 후에 BN을 실해해주면 된다.
- fast optimizer: 최적화해서 빨리 계산하게 해주는 것들, momentum optimization, RMSProp, Adam
- overfitting 문제 해결: Regularization(Dropout, 재학습시킬 때 다 학습시키는 게 아니라 몇 개를 버린다.)
- 최적 모델: Early stopping(validation test 로스가 올라갈 때 training을 끝냄)
- CNN(Convolutional Neural Network): dense layer와 다르게 지역적인 부분만 수용해서 가공한다.
- zero padding: CNN을 사용하다 보면 크기가 줄어들 수밖에 없는데 이를 0으로 채워서 크기를 맞춰준다. stride를 사용해서 몇 칸씩 띄워서 사용하기도 함.
- filter: 각 뉴런마다의 가중치 값을 의미한다.
- feature map: filter를 통해 나온 결과값을 의미한다. 하나의 특징을 갖고 있기 때문.
- pooling layer: input image에 대해서 부분 샘플을 만들어서 메모리, 시간 등을 줄여주고, overfitting 줄여준다.
- RNN(Recurrent Neural Network): 출력값이 다시 입력값으로 들어가는 것을 의미하며 시간성 있는 데이터에 많이 사용된다.
- BPTT(Backpropagation Through time)으로 RNN을 트레이닝 시킬 수 있다. forward pass -> 결과값 평가하기 -> backward pass -> model 업데이트하기
- Layer Normalization: RNN에는 batch norm을 사용할 수 없어서 layer 단위로 norm을 진행
- LSTM(Long Short-term Memory): 데이터 가중치를 안 줄이도록 만들어줌, 오래 기억할 수 있게.
- GRU(Gated Recurrent Unit): LSTM을 간단한 버전, input이나 long-term 둘 중 하나만 다음 출력값으로 넘길 수 있게 한다.
- Autoencoder: 잠재된 표현을 찾아냄, 무언가를 생성할 수도 있음, unsupervised, undercomplete(새로운 거 생성) 또는 overcomplete(noise 추가하고 원래대로 되돌리기)를 사용해서 할 수도 있음
- VAE(Variational Autoencoder): 확률 개념을 추가한 AE, 가우시안 값 같은 걸 추가해서 새로운 거 생성
